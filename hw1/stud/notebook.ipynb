{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import  List, Tuple, Dict\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data: 100%|██████████| 254592/254592 [00:00<00:00, 1067203.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sentences)=14534\n",
      "CPU times: user 292 ms, sys: 2.59 ms, total: 295 ms\n",
      "Wall time: 294 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = []\n",
    "labels = []\n",
    "sentences = []\n",
    "with open ('../../data/train.tsv', 'r') as f:\n",
    "    # strip lines, remove empty ones and the first\n",
    "    lines = list(filter(None, map(str.strip, f.readlines())))[1:]\n",
    "    for line in tqdm(lines, desc='Reading data', total=len(lines)):\n",
    "        line: List[str] = line.split('\\t')\n",
    "        if line[0] == '#':\n",
    "            sentences.append((words, labels))\n",
    "            words = []\n",
    "            labels = []\n",
    "        else:\n",
    "            words.append(line[0])\n",
    "            labels.append(line[1])\n",
    "\n",
    "print(f'{len(sentences)=}')\n",
    "# pprint(sentences[0], compact=True)\n",
    "# print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \"\"\"Implements a vocabulary of both words and labels. Automatically adds '<unk>' and '<pad>' word types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences: List[Tuple[List[str], List[str]]], threshold: int = 1):\n",
    "        \"\"\"Initialize the vocabulary from a dataset\n",
    "\n",
    "        Args:\n",
    "            sentences (List[Tuple[List[str], List[str]]]):\n",
    "                The dataset as a list of tuples. \n",
    "                Each tuple contains two lists: the words of a sentence\n",
    "                and the corresponding labels\n",
    "\n",
    "            threshold (int, optional): \n",
    "                Number of appearances needed for a word to\n",
    "                be inserted in the dictionary. Defaults to 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.threshold: int = threshold\n",
    "        self.counts: Counter = Counter()\n",
    "        self.lcounts: Counter  = Counter()\n",
    "\n",
    "        # unk and pad symbols\n",
    "        self.unk_symbol = '<unk>'\n",
    "        self.pad_symbol = '<pad>'\n",
    "\n",
    "        for sentence, labels in sentences:\n",
    "            for word, label in zip(sentence, labels):\n",
    "                self.counts[word] += 1\n",
    "                self.lcounts[label] += 1\n",
    "\n",
    "                if label == 'id':\n",
    "                    print(f'{sentence=}')\n",
    "                    print(f'{labels=}')\n",
    "\n",
    "            \n",
    "        # word vocabularies\n",
    "        self.itos: List[str] = sorted(list(filter(lambda x: v.counts[x] >= threshold, self.counts.keys())) + [self.unk_symbol, self.pad_symbol])\n",
    "        self.stoi: Dict[str, int] = {s: i for i, s in enumerate(self.itos)}\n",
    "\n",
    "        # label vocabularies\n",
    "        self.ltos: List[str] = sorted(list(self.lcounts.keys()))\n",
    "        self.stol: Dict[str, int] = {s: i for i, s in enumerate(self.ltos)}\n",
    "\n",
    "        self.unk: int = self.stoi[self.unk_symbol]\n",
    "        self.pad: int = self.stoi[self.pad_symbol]\n",
    "\n",
    "\n",
    "    def __contains__(self, word: str):\n",
    "        return word in self.stoi\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "\n",
    "    def getWord(self, id: int) -> str:\n",
    "        \"\"\"Return the word at a given index\n",
    "\n",
    "        Args:\n",
    "            id (int): the index of a word\n",
    "\n",
    "        Returns:\n",
    "            str: the word corresponding to the given index\n",
    "        \"\"\"\n",
    "        return self.itos[id]\n",
    "\n",
    "\n",
    "    def getWordId(self, word: str) -> int:\n",
    "        \"\"\"Get the index of a given word\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to retrieve the index of\n",
    "\n",
    "        Returns:\n",
    "            int: Index of the word if present, otherwise the index of '<unk>'\n",
    "        \"\"\"\n",
    "        return self.stoi[word] if word in self.stoi else self.unk\n",
    "\n",
    "\n",
    "    def getLabel(self, id: int) -> str:\n",
    "        \"\"\"Get a label name from its index\n",
    "\n",
    "        Args:\n",
    "            id (int): the index of a label\n",
    "\n",
    "        Returns:\n",
    "            str: the correpsonding label name\n",
    "        \"\"\"\n",
    "        return self.ltos[id]\n",
    "\n",
    "\n",
    "    def getLabelId(self, label: str) -> int:\n",
    "        \"\"\"Get the id of a label\n",
    "\n",
    "        Args:\n",
    "            label (str): the name of the label\n",
    "\n",
    "        Returns:\n",
    "            int: the corresponding index\n",
    "        \"\"\"\n",
    "        return self.stol[label]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int or str) -> str or int:\n",
    "        if isinstance(idx, str):\n",
    "            return self.getWordId(idx)\n",
    "        elif isinstance(idx, int):\n",
    "            return self.getWord(idx)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path: Path = Path('../../data/train.tsv'), vocab: Vocabulary = None, threshold: int = 2, window_size: int = 7, window_shift: int = None):\n",
    "        \"\"\"Build a Named Entity Recognition dataset from a .tsv file, which loads data as fixed-size windows\n",
    "\n",
    "        Args:\n",
    "            path (Path, optional): Path of the .tsv dataset file. Defaults to Path('../../data/train.tsv').\n",
    "            vocab (Vocabulary, optional): Vocabulary to index the data. If none, build one. Defaults to None.\n",
    "            threshold (int, optional): If vocab is None, threshold for the vocabulary. Defaults to 1.\n",
    "            window_size (int, optional): Size of the windows. Defaults to 5.\n",
    "            window_shift (int, optional): Shift of the windows. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert vocab or ('train' in str(path)), \\\n",
    "            \"Careful, you are trying to build a vocabulary on something that is not the training data\"\n",
    "        self.path: Path = path\n",
    "        self.sentences: List[Tuple[List[str], List[str]]] = self.loadData(self.path)\n",
    "        self.vocab: Vocabulary = vocab or Vocabulary(sentences, threshold=threshold)\n",
    "        self.indexed_data: List[Tuple[List[int], List[int]]] = self.indexData()\n",
    "        self.window_size: int = window_size\n",
    "        self.window_shift: int = window_shift or window_size\n",
    "        assert self.window_shift <= self.window_size and self.window_shift >= 0 and self.window_size > 0, \\\n",
    "            \"Window shift must be equal or less than window size, both must be positive\"\n",
    "        self.windows: List[Tuple[torch.Tensor, torch.Tensor]] = self.build_windows()\n",
    "        \n",
    "\n",
    "    def loadData(self, path: Path):\n",
    "        \"\"\"Loads the dataset from file\n",
    "\n",
    "        Args:\n",
    "            path (Path): path of the .tsv dataset\n",
    "\n",
    "        Returns:\n",
    "            sentences (List[Tuple[List[str], List[str]]]):\n",
    "                a list of sentences. Each sentences is a tuple made of:\n",
    "                - list of words in the sentence\n",
    "                - list of labels of the words\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        labels = []\n",
    "        sentences = []\n",
    "        with open (path, 'r') as f:\n",
    "            # strip lines, remove empty ones and the first\n",
    "            lines = list(filter(None, map(str.strip, f.readlines())))[1:]\n",
    "            for line in tqdm(lines, desc='Reading data', total=len(lines)):\n",
    "                line: List[str] = line.split('\\t')\n",
    "                if line[0] == '#':\n",
    "                    sentences.append((words, labels))\n",
    "                    words = []\n",
    "                    labels = []\n",
    "                else:\n",
    "                    words.append(line[0])\n",
    "                    labels.append(line[1])\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def indexData(self) -> List[Tuple[List[int], List[int]]]:\n",
    "        \"\"\"Builds self.indexed_data transforming both words and labels in integers\n",
    "\n",
    "        Args:\n",
    "            vocab (Vocabulary): the vocabulary to use to convert words to indices\n",
    "        \"\"\"\n",
    "        data = list(map(\n",
    "            lambda sentence: (\n",
    "                [self.vocab[w] for w in sentence[0]],\n",
    "                [self.vocab.getLabelId(l) for l in sentence[1]]\n",
    "            ),\n",
    "            self.sentences\n",
    "        ))\n",
    "        return data\n",
    "\n",
    "\n",
    "    def build_windows(self) -> List[Tuple[List[int], List[int]]]: \n",
    "        \"\"\"Builds fixed-size windows from the indexed data\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[Tensor, Tensor]]: List of fixed-size windows\n",
    "        \"\"\"\n",
    "        windows: List[Tuple[List[int], List[int]]] = []\n",
    "        for word_ids, label_ids in self.indexed_data:\n",
    "            start = 0\n",
    "            while start < len(word_ids):\n",
    "                # generate window\n",
    "                word_window = word_ids[start: start+self.window_size]\n",
    "                label_window = label_ids[start: start+self.window_size]\n",
    "                # pad\n",
    "                word_window += [self.vocab.pad] * (self.window_size - len(word_window))\n",
    "                label_window += [self.vocab.getLabelId('O')] * (self.window_size - len(label_window))\n",
    "                # append\n",
    "                windows.append((torch.tensor(word_window), torch.tensor(label_window)))\n",
    "                start += self.window_shift\n",
    "        return windows\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.windows[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(trainset: NerDataset, devset: NerDataset, batch_size: int = 64):\n",
    "\n",
    "    return (\n",
    "        DataLoader(trainset, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(devset, batch_size=batch_size * 2),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data: 100%|██████████| 254592/254592 [00:00<00:00, 2581902.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Vocabulary usage\n",
      "---------------\n",
      "v.counts[\"is?\"]=1\n",
      "'is?' in v? False\n",
      "v['is?']=605\n",
      "v['<unk>']=605\n",
      "v[605]='<unk>'\n",
      "v[\"605\"]=605\n",
      "v[\"60006665\"]=605\n",
      "v.unk=605\n",
      "v.pad=604\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "d = NerDataset(threshold=2, window_size=100, window_shift=0)\n",
    "v = d.vocab\n",
    "\n",
    "\n",
    "print('Examples of Vocabulary usage\\n---------------')\n",
    "# as an examples, 'is?' has count=1\n",
    "print(f'{v.counts[\"is?\"]=}')\n",
    "print(f\"'is?' in v? {'is?' in v}\")\n",
    "\n",
    "print(f\"{v['is?']=}\")\n",
    "print(f\"{v['<unk>']=}\")\n",
    "print(f'{v[605]=}')\n",
    "\n",
    "# this is equal to 605 because the string \"605\" never figures in the dataset, so it is an unknown token with index 605\n",
    "print(f'{v[\"605\"]=}')\n",
    "print(f'{v[\"60006665\"]=}')\n",
    "\n",
    "print(f'{v.unk=}')\n",
    "print(f'{v.pad=}')\n",
    "print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes: int, embedding_dim: int, vocab_size: int, padding_idx: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.embedding(x))\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data: 100%|██████████| 254592/254592 [00:00<00:00, 2578741.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows[:5]=tensor([[ 4595,  4792,  6109,    14,   604,   604,   604],\n",
      "        [  605,     2,    10,  1003,    78,  3830, 11274],\n",
      "        [ 5110,  5676,   612,  6805,  7452, 10572,  3284],\n",
      "        [ 5110,  7471,  1095,  5470,  6293,  8664, 11521]])\n",
      "labels[:5]=tensor([[12,  3,  9, 12, 12, 12, 12],\n",
      "        [12, 12, 12, 12, 12, 12, 12],\n",
      "        [12, 12, 12, 12, 12, 12,  2],\n",
      "        [12, 12, 12, 12,  1, 12, 12]])\n",
      "word 4792: glen - label n.3: B-LOC\n",
      "word 6109: lake - label n.9: I-LOC\n",
      "word 3284: democratic - label n.2: B-GRP\n"
     ]
    }
   ],
   "source": [
    "# Try the dataset and make sure everything is as expected\n",
    "torch.manual_seed(777)\n",
    "\n",
    "trainset = NerDataset(threshold=2)\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "windows, labels = next(iter(trainloader))\n",
    "print(f'{windows[:5]=}')\n",
    "print(f'{labels[:5]=}')\n",
    "\n",
    "v = trainset.vocab\n",
    "print(f'word 4792: {v[4792]} - label n.3: {v.getLabel(3)}')\n",
    "print(f\"word 6109: {v[6109]} - label n.9: {v.getLabel(9)}\")\n",
    "print(f\"word 3284: {v[3284]} - label n.2: {v.getLabel(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has shape [batches, window_size]\n",
      "windows.shape=torch.Size([4, 7])\n",
      "\n",
      "The output has shape [batches, window_size, n_classes]\n",
      "torch.Size([4, 7, 13])\n",
      "\n",
      "Predicted labels: \n",
      "[['I-LOC', 'I-GRP', 'I-CORP', 'B-PROD', 'O', 'O', 'O'],\n",
      " ['I-PROD', 'B-CW', 'O', 'I-PER', 'B-PER', 'I-CORP', 'I-LOC'],\n",
      " ['I-CORP', 'O', 'I-LOC', 'I-CORP', 'B-PER', 'O', 'B-PER'],\n",
      " ['I-LOC', 'I-CORP', 'I-PER', 'B-PER', 'B-PER', 'I-CW', 'I-LOC']]\n",
      "Gold labels: \n",
      "[['O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O'],\n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'B-GRP'],\n",
      " ['O', 'O', 'O', 'O', 'B-CW', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "model = NerModel(n_classes=13, embedding_dim=100, vocab_size=len(v), padding_idx=v.pad, hidden_size=50)\n",
    "\n",
    "print('The input has shape [batches, window_size]')\n",
    "print(f'{windows.shape=}\\n')\n",
    "\n",
    "output = model(windows)\n",
    "print('The output has shape [batches, window_size, n_classes]')\n",
    "print(output.shape)\n",
    "print()\n",
    "\n",
    "predictions = output.argmax(dim = -1)\n",
    "\n",
    "predicted_labels = [[v.getLabel(i) for i in w] for w in predictions]\n",
    "print('Predicted labels: ')\n",
    "pprint(predicted_labels)\n",
    "\n",
    "gold_labels = [[v.getLabel(i) for i in w] for w in labels]\n",
    "print('Gold labels: ')\n",
    "pprint(gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "def step(model, loss_fn, inputs, labels, opt=None):\n",
    "\n",
    "    output = model(inputs)\n",
    "    output = output.view(-1, output.shape[-1])\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    loss = loss_fn(output, labels)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(inputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, model, loss_fn, opt, trainloader, devloader):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in trainloader:\n",
    "            step(model, loss_fn, xb.to(device), yb.to(device), opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[step(model, loss_fn, xb.to(device), yb.to(device)) for xb, yb in devloader]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data: 100%|██████████| 254592/254592 [00:00<00:00, 530089.29it/s]\n",
      "Reading data: 100%|██████████| 13515/13515 [00:00<00:00, 3056509.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8079369198152186\n",
      "1 0.8059054943638235\n",
      "2 0.8040993473666432\n",
      "3 0.8025113290206782\n",
      "4 0.8019006144317373\n",
      "5 0.8012988969490841\n",
      "6 0.7981761881251873\n",
      "7 0.7992477585559852\n",
      "8 0.7965851146280215\n",
      "9 0.7977408805307752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainset = NerDataset(path=Path('../../data/train.tsv'),threshold=2, window_size=7)\n",
    "devset = NerDataset(path=Path('../../data/dev.tsv'), vocab=trainset.vocab, window_size=7)\n",
    "\n",
    "\n",
    "trainloader, devloader = get_dataloaders(trainset, devset, batch_size=128)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(epochs=10, model=model, loss_fn=loss_fn, opt=opt, trainloader=trainloader, devloader=devloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85c2248f7ac3a0554bc8520d68bee5c21691a08b907f2442a3986db512315a92"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('nlp2022-hw1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
